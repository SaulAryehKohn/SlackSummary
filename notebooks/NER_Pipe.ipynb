{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import itertools\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spent a couple of train rides hand-labeling the relevant topics for a summary on 200 DailyDialog conversations with > 10 turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/first_200_conversations_labelled.csv', encoding='latin-1',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans time series filtering, these are the algorithms employed for the SummaryBot NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_entity_types = ['EVENT','PERSON','PRODUCT',\n",
    "                            'ORG','TIME',# v. relevant to a summary\n",
    "                            'FAC','GPE','LOC', # locations\n",
    "                            'LANGUAGE','WORK_OF_ART' #others\n",
    "                            ]\n",
    "nonlocation_entities = ['EVENT','PERSON','PRODUCT','ORG','TIME','LANGUAGE','WORK_OF_ART']\n",
    "\n",
    "def extract_entities(conv_string):\n",
    "    \"\"\"\n",
    "    Given a string as input, perform Named Entity Recognition.\n",
    "    Extract interesting entities to report on in a summary.\n",
    "    \n",
    "    NOTE: Capitalization is important! \n",
    "          For example, \"Amazon\" will be recognized as an organization,\n",
    "          but \"amazon\" will not be.\n",
    "    \"\"\"\n",
    "    conv_nlp = nlp(conv_string)\n",
    "    dct = {}\n",
    "    # I feel like there's a way to get rid of/vectorize this for loop\n",
    "    for ent in conv_nlp.ents:\n",
    "        if ent.text.strip() == '':\n",
    "            continue\n",
    "        if ent.label_ in interesting_entity_types:\n",
    "            try:\n",
    "                dct[ent.label_].append(ent.text)\n",
    "            except KeyError:\n",
    "                dct[ent.label_] = [ent.text]\n",
    "    dct_ = {k:set(dct[k]) for k in dct.keys()}\n",
    "    return dct_    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the pipe! Extract interesting entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity_dict'] = df['dialog'].apply(lambda x: extract_entities(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the \"payload\"; a string passed to Slack to be printed for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_firstTwoFromDict_string(dct,key):\n",
    "    \"\"\"\n",
    "    Extract first two entries (if there are more than one)\n",
    "    from dictionary value dct[key] and print prettily.\n",
    "    \"\"\"\n",
    "    kstring = None\n",
    "    if key in dct:\n",
    "        if len(dct[key]) == 1:\n",
    "            kstring = list(dct[key])[0]\n",
    "        else:\n",
    "            # take first two\n",
    "            # XXX this could be improved\n",
    "            kstring = '{0} and {1}'.format(*list(dct[key])[:2])\n",
    "    return kstring\n",
    "\n",
    "def create_locationFromEntityDict_string(dct):\n",
    "    \"\"\"\n",
    "    Generate a string for locations, according to relevance\n",
    "    priorities: GPE > FAC > LOC (country/city/state > named places > lakes etc.)\n",
    "    \"\"\"\n",
    "    locstring = None\n",
    "    if 'GPE' in dct:\n",
    "        locstring = create_firstTwoFromDict_string(dct,'GPE')\n",
    "    elif 'FAC' in dct:\n",
    "        locstring = create_firstTwoFromDict_string(dct,'FAC')\n",
    "    elif 'LOC' in dct:\n",
    "        locstring = create_firstTwoFromDict_string(dct,'LOC')\n",
    "    return locstring\n",
    "\n",
    "def construct_entity_payload(entities_dict,conversant_string=''):\n",
    "    payload_dict = {}\n",
    "    for k in nonlocation_entities:\n",
    "        payload_dict['{0}_string'.format(k)] = create_firstTwoFromDict_string(entities_dict,k)\n",
    "    payload_dict['LOC_string'] = create_locationFromEntityDict_string(entities_dict)\n",
    "    \n",
    "    # situation: no entities were extracted\n",
    "    # XXX TODO: LOCATE SUBJECT NOUNS IN SPACY SPAN\n",
    "    if all(v==None for v in payload_dict.values()):\n",
    "        payload = conversant_string+'... something, but I cannot detect what.'\n",
    "    else:\n",
    "        payload = conversant_string+':'\n",
    "        if payload_dict['EVENT_string']:\n",
    "            payload+='EVENT: {0}; '.format(payload_dict['EVENT_string'])\n",
    "        if payload_dict['LOC_string']:\n",
    "            payload+='LOCATION: {0}; '.format(payload_dict['LOC_string'])\n",
    "        if payload_dict['PERSON_string']:\n",
    "            payload+='PEOPLE: {0}; '.format(payload_dict['PERSON_string'])\n",
    "        if payload_dict['ORG_string']:\n",
    "            payload+='ORGANIZATION: {0}; '.format(payload_dict['ORG_string'])\n",
    "        if payload_dict['LANGUAGE_string']:\n",
    "            payload+='LANGUAGE: {0}; '.format(payload_dict['LANGUAGE_string'])\n",
    "        if payload_dict['WORK_OF_ART_string']:\n",
    "            payload+='WORKS: {0}; '.format(payload_dict['WORK_OF_ART_string'])\n",
    "        if payload_dict['TIME_string']:\n",
    "            payload+='AT TIME: {0}'.format(payload_dict['TIME_string'])\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity_payload'] = df['entity_dict'].apply(lambda x: construct_entity_payload(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the NER matches up to the hand labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_topics</th>\n",
       "      <th>entity_payload</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Opera, Carmen</td>\n",
       "      <td>:PEOPLE: Laura and Carmen;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cambridge University, room</td>\n",
       "      <td>:PEOPLE: Smith and Mrs; ORGANIZATION: Cambridg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   summary_topics  \\\n",
       "Index                               \n",
       "0                   Opera, Carmen   \n",
       "1      Cambridge University, room   \n",
       "\n",
       "                                          entity_payload  \n",
       "Index                                                     \n",
       "0                            :PEOPLE: Laura and Carmen;   \n",
       "1      :PEOPLE: Smith and Mrs; ORGANIZATION: Cambridg...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['summary_topics','entity_payload',]].iloc[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary_topics_list'] = df['summary_topics'].apply(lambda x: [w.lower().strip() for w in x.split(',')])\n",
    "df['payload_topics_list'] = df['entity_payload'].apply(lambda x: [w.lower() for w in word_tokenize(x) if not w in stop_words and len(w)>3])\n",
    "df['summary_payload_intersection_test'] = df[['summary_topics_list','payload_topics_list']].apply(lambda x: len(set(x[0]).intersection(set(x[1])))>=1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumtest = 'Opera, Carmen'\n",
    "paytest = ':PEOPLE: Laura and Carmen;'\n",
    "sumwords = [w.lower().strip() for w in sumtest.split(',')]\n",
    "paywords = [w.lower() for w in word_tokenize(paytest) if not w in stop_words and len(w)>3]\n",
    "sumset = set(sumwords)\n",
    "payset = set(paywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.29"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df['summary_payload_intersection_test'])/200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
